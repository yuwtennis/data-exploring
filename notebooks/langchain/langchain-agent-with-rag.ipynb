{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e436cc47-dfe3-49aa-b722-7c8188453a02",
   "metadata": {},
   "source": [
    "# Agent with RAG\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021dab2-be3d-4a47-961e-74f798067923",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6ec26-8cfb-440d-acc1-fe6ab2bbf7e0",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e5d947-0913-4b80-8bfc-0fae43bd7119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph beautifulsoup4 langchain[google-genai] langchain-google-vertexai langchain-google-genai langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21fac4-f1cb-4621-8f00-84e792aa3349",
   "metadata": {},
   "source": [
    "### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5da6194-36af-4323-8390-51f53d0a630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a37e04-6bc5-4783-b37c-91763270b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for Google Gemini:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26666b3-8a0f-4f7b-9dc9-59cdd6646ffa",
   "metadata": {},
   "source": [
    "### Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "909d51a8-a08c-492d-9473-1b1e075c4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat_model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b2a60-4913-45a6-9dc4-4767ced57f62",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31870fff-f398-4747-8e38-c5444657724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433efaf5-d68a-4198-a13e-1827720d9728",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2c3c849a-74fe-4526-829b-0eeee0e3e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "vector_store: List[Document] = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d8066-03ab-4a84-bc73-957c45d80078",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4876fc-657a-484d-a349-9c87109f4819",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f5b90-d188-4436-8faa-9b0500f0f549",
   "metadata": {},
   "source": [
    "#### Load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "eb90819d-c4d5-458c-8d56-40ff7e0faaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "INDEX_URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "\n",
    "# Load\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(INDEX_URL,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4c79b171-0d5b-427a-a168-5eadff373d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e9e95-226a-4cbe-9a79-10d69d01564e",
   "metadata": {},
   "source": [
    "### Tie retrieve and generate tasks in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b1b2dcf4-d72b-498d-8285-e86e165b3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c652-3a4a-4264-aae0-5966ee590d31",
   "metadata": {},
   "source": [
    "### Retrieval step as tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "841c68ef-6baa-46e3-82c7-624f91b4cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Tuple\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str) -> Tuple[Tuple[str], list[Document]]:\n",
    "    \"\"\"Retrieve information related to user query\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    print(f\"Serialized: {serialized}\")\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "86334efa-2aab-4064-b5f7-9c6b5fac1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from langchain_core.language_models.chat_models import BaseChatModel, LanguageModelInput\n",
    "from langchain_core.messages.base import BaseMessage, BaseMessageChunk\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Callable\n",
    "\n",
    "LLM = chat_model\n",
    "RETRIEVE_TOOL = retrieve\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include tool-call to be sent\n",
    "def query_or_respond(state: MessagesState) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "    \"\"\"Generate tool call for retrieval or respond\"\"\"\n",
    "    llm_with_tools = LLM.bind_tools([RETRIEVE_TOOL])\n",
    "    response: Runnable[LanguageModelInput, BaseMessage] = llm_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "21513f5b-66e9-4e70-9130-a3571e1a4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tool as node\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f763f6ca-b0e7-477c-933c-3d294e861327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate a response using the retrieved content\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\" Generate answer \"\"\"\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        print(f\"State: {message}\")\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\") or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "    \n",
    "    # Run\n",
    "    ai_message = LLM.invoke(prompt)\n",
    "    return {\"messages\": [ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d35f3129-6e55-43f9-b0ea-2bc10987071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile it\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9a7aa67b-c74b-42c4-9787-1fc3f26e7846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30d8ff-3b4d-478b-976d-36d3dbfb9947",
   "metadata": {},
   "source": [
    "## Testing the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e79a93b5-b859-4054-b85d-b4d9f1c7190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "input_message = \"Hello\"\n",
    "\n",
    "stream: Iterator[BaseMessageChunk] = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\"\n",
    ")\n",
    "\n",
    "for step in stream:\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fb298ad5-bd7e-4ca4-8433-b008150a49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (b1be361f-3a2f-45ab-be3e-41435e9cfbe9)\n",
      " Call ID: b1be361f-3a2f-45ab-be3e-41435e9cfbe9\n",
      "  Args:\n",
      "    query: What is Task Decomposition?\n",
      "Serialized: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "State: content='Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#' name='retrieve' id='6f8d5cf3-39eb-4d16-9ce7-9402c2fe3312' tool_call_id='b1be361f-3a2f-45ab-be3e-41435e9cfbe9' artifact=[Document(id='d3b07d44-866c-4991-a3aa-5f62437ef9e1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='869142d6-8bfc-46c2-828c-4e757084f458', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#')]\n",
      "State: content='' additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{\"query\": \"What is Task Decomposition?\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--c97bc065-80d6-42bd-be8c-a5a81abdc587-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'What is Task Decomposition?'}, 'id': 'b1be361f-3a2f-45ab-be3e-41435e9cfbe9', 'type': 'tool_call'}] usage_metadata={'input_tokens': 16, 'output_tokens': 7, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition breaks down a complex task into smaller, simpler steps. This can be achieved through prompting the model to \"think step by step\" or by using task-specific instructions. Another approach involves using an external classical planner to do long-horizon planning.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51508b4d-d0af-4eb4-b6bd-295c03f9154a",
   "metadata": {},
   "source": [
    "## Stateful agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a331-179b-45a6-9077-bb8b865c66ab",
   "metadata": {},
   "source": [
    "### Memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f13896ef-785c-4a02-bb9c-0c7b8fac31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify threadid\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "1eb64f1b-516f-4494-a399-eb5b022a0c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is Task Decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (613e762d-04ec-4aba-a3f4-0a37fddd2c2e)\n",
      " Call ID: 613e762d-04ec-4aba-a3f4-0a37fddd2c2e\n",
      "  Args:\n",
      "    query: Task Decomposition\n",
      "Serialized: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "State: content='Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#' name='retrieve' id='02245c99-1479-49a9-97cd-29dd7a2e5078' tool_call_id='613e762d-04ec-4aba-a3f4-0a37fddd2c2e' artifact=[Document(id='d3b07d44-866c-4991-a3aa-5f62437ef9e1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='869142d6-8bfc-46c2-828c-4e757084f458', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#')]\n",
      "State: content='' additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{\"query\": \"Task Decomposition\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--be031753-7ca8-48eb-92ab-972713213a6e-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'Task Decomposition'}, 'id': '613e762d-04ec-4aba-a3f4-0a37fddd2c2e', 'type': 'tool_call'}] usage_metadata={'input_tokens': 16, 'output_tokens': 4, 'total_tokens': 20, 'input_token_details': {'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is when a complex task is broken down into smaller and simpler steps. This can be achieved through methods like chain of thought prompting, task-specific instructions, or human inputs. It can also be outsourced to an external classical planner using the Planning Domain Definition Language (PDDL).\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "fe0a64b8-960a-40ca-8d1e-9e72e79648e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you look up some common ways of doing it?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (b8b41576-35ee-4968-85a1-1ba4fec55403)\n",
      " Call ID: b8b41576-35ee-4968-85a1-1ba4fec55403\n",
      "  Args:\n",
      "    query: common ways to do task decomposition\n",
      "Serialized: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "State: content='Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#' name='retrieve' id='ecdba45d-bbb7-4364-9829-ce275773aada' tool_call_id='b8b41576-35ee-4968-85a1-1ba4fec55403' artifact=[Document(id='d3b07d44-866c-4991-a3aa-5f62437ef9e1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='869142d6-8bfc-46c2-828c-4e757084f458', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#')]\n",
      "State: content='' additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{\"query\": \"common ways to do task decomposition\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []} id='run--59ae0fe7-aa95-4aba-b503-60feab7f6bdb-0' tool_calls=[{'name': 'retrieve', 'args': {'query': 'common ways to do task decomposition'}, 'id': 'b8b41576-35ee-4968-85a1-1ba4fec55403', 'type': 'tool_call'}] usage_metadata={'input_tokens': 584, 'output_tokens': 8, 'total_tokens': 592, 'input_token_details': {'cache_read': 0}}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition can be done by LLM with simple prompting, by using task-specific instructions, or with human inputs. Another approach is LLM+P, which involves relying on an external classical planner to do long-horizon planning.\n"
     ]
    }
   ],
   "source": [
    "input_message = \"Can you look up some common ways of doing it?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecb8f5-600e-4dd6-8120-2125ded08e1a",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "789d71ff-ca53-42ed-88fa-f081c043eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(LLM, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "04233409-64ce-4e94-8716-8538fc8e5183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAQAElEQVR4nOzdB1hUV9oH8DOdKbQZuhTBggq2KBqJseGaaOwFYovly+pqTHRj2TWuMZtsTNyY1dUYjYnGEhVFBHvUGIWIEMUICgIqCJHeZ5hevxfHEBYBlcwdzp05v8dnnjv33kHKf0699wzbZDIhgmhvbEQQGCBBJLBAgkhggQSRwAIJIoEFEkQCCySITWnVhsoirbLOoKzTG/QmnZYGw1s8PpPNZQgc2QJHpqc/H9EQg4wjminl+ns35HkZiupSjYsHV+DIgr+rk5it09Dg98NxYNaUwptHD3EsyFIGhYqCegk79RIh+iBBRPAbuHqyqjRf5e7nEBQq9O0iQHSmVRvzMuQPc1RF91Xh4yRdX3BEdGDvQcz6WXYxuhz+YC+McEW2pa5GB28wKCZHzfYSOuHeBrPrICYeq2Bx0Evj3JHtqi7TxG8rHjnD078b1iW9/QbxUky52JPbe4gLsgPHdxS9OEbi6e+AcGWnQTy5s9gvWNBnqF2k0Oz49qJuYU7B/TFtMjKR/bl6stKnE9+uUggmLOrwy481lcUahCW7C+K9m3Xw2C/C1romz2L6Kn9oFpuMONaBdhfEhNiKvsPtMYVmQT1FV45XIvzYVxBvXq7p1t+JL2IhewUNkns35QqZHmHGvoKYn6kYNE6M7NuQyW5pCbUIM3YUxPw7CjaHyWLZY/+sMf9uwowkKcKMHf1VHtxWBPYUIuv6+9//fvz4cfT8/vSnPxUVFSEKcB2Y7r48mABEOLGjIFaXaztZPYh37txBz6+kpKSmpgZRpmtfUeF9JcKJvQRRqzZWFmn4IqqmXJOSkhYuXDh48OCJEyeuW7eusrK+Z9q/f//i4uKPPvpo2LBh8FQul+/YsWPOnDnm0zZt2qRWq80vj4iIOHTo0J///Gd4SUJCwrhx42DnhAkTli9fjiggdOZUFOI1oGgvQYR+InUT/9nZ2UuXLg0LCzt69OiqVavu3r37wQcfoEfphMe1a9devnwZNqKjo/fs2TN79uzNmzfD+RcuXNi5c6f5K3A4nLi4uODg4G3btr300ktwAuyEOv3zzz9HFBA6sRQyA8KJvVwYq5Dqhc5U/bBpaWkODg7z589nMpleXl49evS4f//+k6fNmjULSr7AwEDz0/T09KtXr77zzjuwzWAwnJ2dV6xYgawCfhXwC0E4sZcgGo2Iy6eq+O/Tpw9UssuWLRs4cOCQIUP8/Pyghn3yNCj2kpOToeKGIlOvr8+BWPz7WBLEF1kLk82ALgvCib1UzVAZSSt0iBrdunXbsmWLu7v71q1bJ02atHjxYijtnjwNjkJdDCfEx8enpqbOmzev8VEul4usRVGrZ7EZCCf2EkSBE1tJ5XRCeHg4tAVPnjwJrUOpVAqlo7nMa2AymWJjY6OioiCIUH3Dnrq6OtROKG0xt429BJEvZLl14Ol1RkSBGzduQGsPNqBQHDt2LHR1IWQwBNP4HJ1Op1KpPDw8zE+1Wm1iYiJqJxql0cOPh3BiR+OIMMWcd1uBKAAVMXSWjx07BoN/GRkZ0DuGRHp7e/N4PEheSkoKVMTQj+nYseOJEycKCwtra2s//PBDaFnKZDKFoplvCc6ER+hWw1dDFLj7S51nAF4XydpREANDhQ8yKAkidIehwt24cSNMhyxYsEAoFEJbkM2ur/ugK339+nUoI6E4XL9+PXSup06dCoOIAwYMWLJkCTwdOXIkjDU2+YK+vr4wlAiDjtCsRBTIv6MMDLH22H7r7OgKba3GeHpXyaTFHZB9+zVHmXdbPmyqB8KJHZWIXB7Tw5f3y48UTp3RwtUTlSGDnBFm7Gulh/Cxkm0rclu6c9RoNI4YMaLZQ9C3gFFAGHZ+8lBQUNDu3bsRNWCoHDrg6Dm/pa5duzbM2TQBrUNXT657B7x6KsgOb55KT6w1Gk19hzWfxZaGVDQaDfQ8mj0EURCJKFxToQ3fEnSMoJ3a7KHTu4pfnuTuJOYgzNjjXXxndpcE93ek14ocFoHzD26PV4mOme+dfKqq/KEa2ZOE2AqJNxfbt5+d3tdcP8/x38IXX5PQfaWbZwQp9PDndQ9zQriy0+vmoWE3dZnf9fM1mSnYXTRvWfCWO769yEnMxjmFiCzClHy68kGmEnrTHXvgNcBrEakXqjNTZMMjPfyDcS/4ybJ0qKpYc/VUFY/P7NCFD/MNAkfaD2lVFGoKshQ3Ltb0etll4Ggxk4nXhTbNIkF8rChXlXO97kGmwtWTI/bkCp3ZQie20JllwOtC5uYxGKa6ar1CZjAZTXd/kTsImZ17iyCFuF102AoSxKZK81UVRVqFFP6ueihLlHWWTCLMOOfl5YWEhCCLErmykan+mktHV7ZPJ76jK3bDhE9FgmhVubm5q1evPnLkCCL+F1nMncACCSKBBRJEAgskiAQWSBAJLJAgElggQSSwQIJIYIEEkcACCSKBBRJEAgskiAQWSBAJLJAgElggQSSwQIJIYIEEkcACCSKBBRJEAgskiAQWSBAJLJAgElggQSSwQIJoVQwGo+ETLojGSBCtymQylZeXI+IJJIgEFkgQCSyQIBJYIEEksECCSGCBBJHAAgkigQUSRAILJIgEFkgQCSyQIBJYIEEksECCSGCBBJHAAgkigQXygT/W8PrrryuVStjQarVVVVXe3t7o0UfQnzt3DhGP2OnH5FrZhAkTSktLi4uLKysr4Z1f/IijoyMifkOCaA1QIvr7+zfew2AwBg8ejIjfkCBaA8Ru8uTJLBarYU9AQEBUVBQifkOCaCWRkZF+fn7mbcjl0KFDzS1FwowE0UrYbDZU0DweD7Z9fX2nTp2KiEZIEK0HameIIGyEh4eT4rAJMo7YlNFoqq3QySp1RgrGtcZFvHnBeGHYgKi8DAWyNA6HIfbmCp1o+Tcl44j/I+dGXUaSVCk3+AQKFDI9ohW+I+vXLIVngMOwqe4iF5rFkQTxd9mpspwbimGRXkwmA9FWTbkmMaZ00lsdhM50yiJpIz6We0uedU0+4nVvWqcQuHrwxi703/tRPqIVEsTHbv1U+9IEG1mVhsVmDBjtfu1cFaIPEsR6aqWholDLF9lO1w3aiCUPNIg+SK+5nqxK5xXARzbEUcI1GujU+idBNGMo6mjWR26dyYAUUjr9RCSIBBZIEAkskCASWCBBJLBAgkhggQSRwAIJIoEFEkQCCySIBBZIEAkskCASWCBBJLBALgOjgbj4I59sWIdsGikRaSAn5w6ydSSIbSSXy2OOfnftenJ+fq5E7BYePnT+vEUODg6o/j5A43+3bLiSdJnL4UZEvBoa0nv1mmWxMefEYoler9+1+8uUn6+Ul5eGhvaZNCHyxRcfLzwycfLIeXP/IpXW7t23k8/nh/UftOStFRKJ27J3F6Sn/wInnD9/+uTxyyKRCNkiUjW30bG46IOH9kRFzl7/8eaFC5deTrgAATIfijl64OSpY28vWbljx3d8vgCSBzuZzPpf9Zat/z4ae3DSxKiDB04OHRKx7p+rEhIvml/F4XAOH94Hp8XHXdz7beztjLQ9e7+C/Zv/s7N799BRo167dDHVVlOISInYZpHTZkGSAgICzU8zMtKvXb+6cME7sH3u/KkhL48YNnQkbM+cMQ/2m8+pX4fu/KkZ0+eOHzcFno4ZPQFetW//1/B1zCd06OA3a+b8+i2RI5SId+9mIbtBgthGUIBdT03+dMO6+7l3ocKFPa6uYng0GAz5+XmjXx3fcOaQlyNu3boJGxAsrVYLCWs41Kd3v7Pfn5DKpM5OzvC0a9fuDYccHZ0UCjmyGySIbbTz661nzsRDpQzB8vT0+mbXtjNnj8N+uUJuMpkEAmHDmc7OLuYNubwOHt9e+n9NvlRNdZU5iAwGve9k/SNIENsConbyVOzUKTPGvjbJvMccMiDgC+BRp9M1nFxT8/i2TombOzwuf3cNVMGNv5qHhxeyeySIbQH1r0qlcnN7fB80VLhXkxPN21Ble3h4Qle64eSkqwnmDd8O/ubVwPr26W/eU1NT/aj4FCC7R3rNbcFms/39O0Lzrqi4EAZc/r3xw56hferqZApF/dJK4YOGnL9w+npqCoQMetCw3/wqCNzcOQuhd3L7dhpkF/rLK1Yt3vzfT5/630EJmpWV8cvN6/AqZKNIENto7Zr1DjyHufOmznpjYr8XBrz55hJ4OmnKyJLS4jlvLOjZs++qvy2Z/cakgoIHUIOj+uxy4PH1qDdWrnj/YPSecROGwVijj7fv8uX/eOr/Ne61ydB8XLnqLaXS8muIYYIswlSv/KHmYnT52AV+yBLUajWMV0ORaX4afXjfgQO7T564jKxIWqm7fLh41nsBiCZIiWh5kLwFf5kZeywaau0fL50/EvPd+PFkfdinIJ0Vy5s7Z4FUWnP+/Kmvv9nq7u4J8ygwrI2IVpEgUmLpO39DxPMgQSSwQIJIYIEEkcACCSKBBRJEAgskiAQWSBAJLJAgElggQSSwQIJIYIEEsR6TxXAS29SvwmQ0ib14iD7I1Tf13Hy4+XcURqPtXBFXVaJmc+l0BwwJ4mPdwpxKHiiRragu1QSG0ukOBBLEx0ZEuV85VqaS28LH/ty8VGUymLr0cUT0Qa7QrpeTkyOTyXr37Lf/44Lew8QiF46LB9dkRPQCTYvKInVVsRoZTSNep9kHXJIgovv377///vu7d+82r1yT+kN14T0VMjGkFZa/U8loMul0Oh6Xiygg9uFl52RUqjL9urM7PtKtWzc2mx6dMLsOYmFhoa+vb25ubqdOnZBVwP+1evXqI0eOIGrAFz937hyDwXB1dRWJRDwez8fHp2vXrosWLUJ4s98gXrly5bPPPjt+/Diyorq6uhs3bgwbNgxRIzs7e9myZZWVlY13Go1Gb2/v06dPI4zZY2dFLq9fUwYyYeUUovoVbRypSyGAurh79+5NdgqFQsxTiOwwiCdOnPjkk09gY/To0cjqKioqvvzyS0SlGTNmQL3c8JTJZP70008Ie3YURHMjBDrIH3/8MWon0De/fPkyolJYWBg0ec0/LFTKQUFB1i/428BegnjhwoX4+HjYWLlyJWo/Hh4eixcvRhSLjIx0dq5fXszPzy86Ojo9PX39+vUIb3bRWcnLy9u5c+ennz59lRmbMXPmzLKysh9++MH8NDY2Ni4u7rvvvkO4svEgJiUlQfsd2kmNm03tCNqIMTExVigUn5SVlTV79uy9e/eGhIQg/Nhy1Xzx4sXDhw9LJBJMUois0kZsCfSmU1NTN2zYcPToUYQf2ywR7969C6O4t2/f7tmzJ8IJ1eOIzwIGDbRa7bp1eH1wiw0GEWqfgoICmLVDRAtgDOvAgQP79+/nUjPZ2AY2VTXX1NSg+kXVXbFNoRXGEZ/F+PHjYQxr6NChaWlpCA+2E8Svv/7a3EmE3zLCVTu2EZvo3LlzcnLy1q1bDx48iDBgC0HU6XTFxcUGg2HatGkIb9YZR3x2u3btKikp+cc/nr5qLdVo30aEBqkSEAAADmZJREFUN/SAAQP8/f3xae7QztmzZ6E+gSYjzEqjdkLvEhHmS+ANDbUMXVKISRuxCZh237RpEzxev34dtRO6BvH8+fPwCKMzy5cvR/SBTxuxiYCAgMTERKipYcwBtQdaBvGLL76AMULY8PKi2Ufl4NZGbGLHjh1SqXTVqlXI6mjWRszOzoYpu1u3bvXq1QsR1IAZqc2bN0OT0cXFBVkLnUrEtWvX3rlT/xHa9E0hnm3EJiIiIr766qspU6bATD2yFnoEEUaqVSrVoEGDJk+ejOgM2zZiEz4+PuaZ+m+++QZZBQ2CCHOjRUVFfD5/zJgxiOYwbyM2sWXLFhij/etf/4qoh3sbMSEhAaqzqVPJB+a0G+hNw3wgNBnhXYQog2+JCF1jeBw4cKAtpZAWbcQmhgwZcuDAgTlz5qSnpyPKYBrEY8eOVVdXw4b5pnebAT/OzZs3Ed24ubnB7Mu2bdugjYSogWnVrFar2Y8gmwOtLr1ez2AwaPce69+/P0y9wHeOKIBpiQh/JJtMIXr0yeLQ8YIOKUxOIvqAEdzg4GCKUoiwDSIMqELtjGwXNLmWLVuG6CMrK+vJW/ctCNMgarVaqMKQTYNCER4fPnyI6ACmEnr06IEog2n1B2NX1NUCWIHxKShp+vXrh/AGJSKlswmYlojQkLLVNmITs2bNgg4pwh60Ee2xarb5NmJj5gukU1JSEK6gXqY0hYi0EfFRWFh47tw5hCWqeyqItBHxARNIMTExCEtQIlJ9hzhpI2LEfPPXoUOHEGasUCKSNiJ2JBIJVquCGI3Ge/fuwWg2ohJpI2Jn1KhRHTt2RNigegTRDNMgQhtxypQpyF7BrC48YrJehRXqZUTaiDibNGnSgQMHUHuz6yDacxuxQd++fYcPH47am11XzfbcRmzMx8cHPSoaUTvR6/UPHjzo0qULohhpI9LAjh079u/f33gPdGiQVVinOESkjUgLnp6eUVFRcrlcpVLB0zFjxlRVVb333nuIetZpICJsZ1agjejv70/3m0ctiPvI4MGDXVxcysvLYdopMzOzurpaLBYjKkGJGBYWhqhH2oh0AmPdpaWl5m1IoRU+ycdqJSKm96xACuFNT2rnxqDRXFBQ0PAUJjzCw8PN9zpSBIqDoUOHJicnI+qRNiI9QMcZeq8QvoY9TCYTcpmXl4coY7WeCiLjiHQRFxcHWYSpP/PCSOZElpWVUVo7W61eRth2Vkgb8Ulr166Fx1u3bv30CHScpTXKhIvXJo+fiaiRk/krDKrX1ehRW0G7z0n8TBnDq404YsQIqVTa8C1BMxG2vby8zpw5g4hGUi9U37pSY2To9RoTn7L7o2E0m8Vm/5HLQl29eUX3lJ17CweOkTiJOa2ciVeJCK1vyBy0fhr2wPa4ceMQ0cj3e0tFYs7o+f4iFw7Cnl5nrC3Xxvy3cPJbHVw9WlxhGq824vTp082TWg18fX1hJyJ+c3ZPqasXr/cQCS1SCNgcplsHh8h3A+O2FcmqW2xu4RXEkJCQ0NDQhqdQNb/66qvWXLcUc/l3FFw+q8eLuHy04HMZHuWdcqa6paPY9ZrfeOMNNzc38zYUh5GRkYj4TflDDYdH1/X3XT1599PqWjqK3U8FA1cNKxOPHj0anw8WxYFGaXDz5iF6YrEZ/sHC2gpts0dxfHvNnTsX5rKgs0yKwyYUMoOezoNa1WXalm7O/KO95uJcpbRSr6jTK2UGowE6/EZkAZLBwYuEQmHqWQ2M2qI/jMdnMhBD4MSCfxIfnrsPXQsVG9bGIBZkKe7+Is/LULh68U0mBovDYsI/FstSo5KhvYbBY50CWYRcyTAaDIYivUGr1qmlOrWhUy9ht/6OngE2tQoorT13EEseqBLjqjgCLoPN6zTIlc1hIbrRqvRVlYqE+Bq+AL08UeLiTj7Er/09XxB/OFRRnKeWBIqFrjQuS7h8ttjPGTZk5YrYrcXdBziGj5Ugol09a2cFxsf3fFigNvD8X/ChdQobc/IQdhrkV17KhLFWRLSrZwqiQW/auTrPu4enSNJuH6NKHZcOThxnp+iN9Fgw01Y9PYhGo2n7qtweEYE8IT3mlNpAJBE4dRDv/VcBItrJ04N44JNfu4R3QLZO4OIg9nM5vYtOC6zbkqcE8XJspYufC09oF/1KRw+RDvHSEmoRYXWtBbGqWPMgQ+HoLkJ2w8XH+Up8Jb0+Otg2tBbExPgqt0Bq71bEkFdX15/iqxBhXS0GsTRfpTcwHd0FCEtpt39YsXagXFGDLM2to0tRnkajMiDikYmTR+7bT/mH5bYYxPvpCpi5Q/aJwczPVCKb8M8P/37m7HGEvRaDmHtL4eiBaXFINYFYeC9NjmxCTs4dRAfNT/HVlGv5jhzqOsv5v946f+mbh4V3RELX7sGDRw1/08Ghfqg8KSXmQsLuRfO374teXVae5+3ZeUj49LAXxppfder7ranpZ3hcQd9er3i4+SPKOHkISjJliP6GR9Qv+PnZxo+279h08vhl2E5KSti7b2fBrw+cnV06dw5e+vbfPD29zCe3cqhBys9Jhw/vy87JFIvdQkN7L3jzbYnEDVlC8yWivFavVlnkgq5mVFY9/GrP2zqdZsmCb+bM2FBSdm/77kUGQ/09iyw2R6Wqiz+9MXLie599mNIrdMSR+H/V1NYvsnH1WuzVa0cnv7Zy6cJvJa4+Fy7tQpRhMBjyGp1C1vbbKDHx/ZkkeFy5Yq05hak3fn7/g5WjRr12JPrMurWflpWVbN7yqfnMVg41uHsve/V7S/v2Dduz++g7b6/Kzb274d8fIAtpPohKmYFF2WU1v6R/z2Zx5k7f4One0csjaNqENUUlORlZCeajBoPuT8PfDPDrCWno3+c1GEkpKrkL+68kH+kVEgHRFAicoIzsHNQfUYnrwFJIaR/EJnZ/u33IyyOmTpkBZV5ISK/Fi95NSbmS/ajubuVQg4zbaQ4ODrNmzoeScuCA8M8/2z59+lxkIS0EsU7P4lJ1pynUy36+PYTCx7dEiV29JWLfBwVpDSf4dwgxbwj4TvCoUtdBHCurH3p6BDac4+vTDVGJw2cp6V8iNpGXd69bt5CGp8Fd65cTyc7ObP1Qg9CefdRq9eo1y2KOHigsegiR7dvHYsVBi2ljIKoGdVVq+cOiOzD40ninrO73obsnryZXaxRGo4HH+73zxOXyEZWMhvrvA9kQuVyu0Wh4vN+vnBII6n+fSqWilUONv0LXLt0+/WRLYuLFnV9v/XL7pn4vDJg7ZyG0FJElNB9EgRPboFMjajg6SgID+rwyYkHjnUKhcysvceAJmUyWrtG3pNFSO7xi0BqETja1CpTDowUh1GpVwx7Fo5xJxG6tHGryRaBGhn/z5v7lxo2fY48dem/NsrhjP7BYFmjFNV81CxxZBh1VI7o+nl1qpaVBHft2Dupn/icSuXq4dWzlJVBGurp45/96u2FPVk4SopJWbRA40e/i81aw2ezgrt0zM2817DFvB3Xq0sqhxl8hLe3Gz9euwoabm/srr4x9a/HyOnldZWUFsoTmg+gkZnO4VFVMMCJjNBpPnN2k1arLKwpOnfvi8y9mlJTdb/1VvUNH3r5zCSZUYPvHn/YVFGYgyhiNJpEL2wZKRB6P5+7ukZqacjMtVa/XT5oYdSXpcmzsIVmdDPZ8uf0/L/QN69K5/iOlWjnUICMz/YN/rjp56lhtbc2drIxjcdGQSPiHLKH537WzG1evNqjrtA6Olh9KhG7viiUHL/20f/OOOeUV+f6+IdMmrnlq52Pk0HkKRU38mc+/O7IGavbxo5cdjHmfoqsTZGUKVw8bmVWaOWP+t3t2XLt+9dDBUzA6U1FZfjhm/xdffg493/79Xvzzm0vMp7VyqEHktFkQwS+2bfzPpvVcLnfE8Fc2/WenRepl1MpqYMmnqwrzTe5B9nh/e3FmeViEqEtfR4SZ7/eW+nQSBfak6/VQcVsLJvzFx9mtmTd5i1N8nXsLTXpbG794RgyGITDEBm+KwFmLzSB3Xwe+wCQtUzh7Nv8nqZWWb/yi+XW6+DyRStP8XK2Xe9CSBV8jy/nHxxEtHYLZGharmR8QGgML5mxp6VUVeTWBPfhsLl2XmKGp1trjQya7Hd1c1FIQHUXidxfvb/YQ9EK43Obv9GMyLdwDaOl7qP82dBoup5lFHdjsFhu+RoOx4oF02ludEGFdrcXCWcLpPlBUVVHn6N5MawkKG7GrD2pvlv0eZCXSYdMsM4tPPJenVEDhY92UlXJlLVWD21iRlshEQmOPgc6IsLqnt4Si3vX99WapTm3jHZfaUrmqWj5yhgci2sMzNckXbgi6l/TQhstFaakcqRWvr/BDRDt5piDCDNvijZ1lRdWysjpkc2oe1nAZqomL2r+9a8+eY5ACCgyJxJCXUigrt9Byce2tpkiWfbkgMJg9eq4XItrV8w2mvDRO0mOgY2JcVWWu0sTiOLkL6bgOiUqmqatQGjUaNx/OmA8CeHyburiBpp57VM/VgzthoXdpvvpemjz3VhlPwDYaGSwuq36tTjb8RXG8NR2aFnqdwajV67UGrUrH4zO79BF1fcGdrIyIjzYOL3t1dIB/L090qy7VSivrb+9QSPUGvdGgxzGIXAcGk8UUOgkETiy3DlyRs73eJouxPzrPIfbiwj9EEH8M+ShaOhE6s2m96IHYi9dS441M7dMJX8isLNIgetJpjYV3Fc5uzdefJIh04hngoNPQdVGe6lJNK5d4kiDSiV9XAYOBbv5Iy8XKfjxY/NL4FhfNx+vzmolnkXisQqczderlJPGhwar6MKIirdBcii6dvcZf2PJ4BQkiLWUkSzOvytRKg4aylWEswr0Dr7ZcG9hT+NI4t9Y/zpIEkcbgT6dVYx1Ek9HkIHymiSsSRAILZByRwAIJIoEFEkQCCySIBBZIEAkskCASWPh/AAAA//8q66zzAAAABklEQVQDAF2nAzPHz8UhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "83e92fb6-5f54-4cc5-b975-688b4cf97650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve (ef4deab7-a7cd-4776-96b5-850b2d1030c5)\n",
      " Call ID: ef4deab7-a7cd-4776-96b5-850b2d1030c5\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "Serialized: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, according to the retrieved information, Chain of Thought (CoT) is a standard prompting technique for task decomposition, where the model is instructed to \"think step by step\" to break down complex tasks into smaller, simpler steps. Other methods mentioned are Tree of Thoughts, which extends CoT by exploring multiple reasoning possibilities at each step, and LLM+P, which relies on an external classical planner. Task decomposition can also be done by LLM with simple prompting, using task-specific instructions, or with human inputs.\n",
      "\n",
      "Now I will look up common extensions of the Chain of Thought method.\n",
      "Tool Calls:\n",
      "  retrieve (f34d4e5d-9d54-4287-b4c5-ae0c9f3e55bb)\n",
      " Call ID: f34d4e5d-9d54-4287-b4c5-ae0c9f3e55bb\n",
      "  Args:\n",
      "    query: common extensions of Chain of Thought method for task decomposition\n",
      "Serialized: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition is Chain of Thought (CoT), where the model is instructed to \"think step by step\" to break down complex tasks into smaller, simpler steps.\n",
      "\n",
      "A common extension of CoT is Tree of Thoughts, which explores multiple reasoning possibilities at each step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search), with each state evaluated by a classifier or majority vote.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aab8b2-1a4e-4549-8d84-b1cd21960aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
