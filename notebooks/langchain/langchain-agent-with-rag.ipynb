{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e436cc47-dfe3-49aa-b722-7c8188453a02",
   "metadata": {},
   "source": [
    "# Agent with RAG\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/qa_chat_history/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021dab2-be3d-4a47-961e-74f798067923",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6ec26-8cfb-440d-acc1-fe6ab2bbf7e0",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5d947-0913-4b80-8bfc-0fae43bd7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph beautifulsoup4 langchain[google-genai] langchain-google-vertexai langchain-google-genai langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad21fac4-f1cb-4621-8f00-84e792aa3349",
   "metadata": {},
   "source": [
    "### Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da6194-36af-4323-8390-51f53d0a630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a37e04-6bc5-4783-b37c-91763270b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26666b3-8a0f-4f7b-9dc9-59cdd6646ffa",
   "metadata": {},
   "source": [
    "### Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d51a8-a08c-492d-9473-1b1e075c4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "chat_model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b2a60-4913-45a6-9dc4-4767ced57f62",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31870fff-f398-4747-8e38-c5444657724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433efaf5-d68a-4198-a13e-1827720d9728",
   "metadata": {},
   "source": [
    "### Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c849a-74fe-4526-829b-0eeee0e3e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "vector_store: List[Document] = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d8066-03ab-4a84-bc73-957c45d80078",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4876fc-657a-484d-a349-9c87109f4819",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f5b90-d188-4436-8faa-9b0500f0f549",
   "metadata": {},
   "source": [
    "#### Load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90819d-c4d5-458c-8d56-40ff7e0faaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "INDEX_URL = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "\n",
    "# Load\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(INDEX_URL,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79b171-0d5b-427a-a168-5eadff373d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e9e95-226a-4cbe-9a79-10d69d01564e",
   "metadata": {},
   "source": [
    "### Tie retrieve and generate tasks in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2dcf4-d72b-498d-8285-e86e165b3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c652-3a4a-4264-aae0-5966ee590d31",
   "metadata": {},
   "source": [
    "### Retrieval step as tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c68ef-6baa-46e3-82c7-624f91b4cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from typing import Tuple\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str) -> Tuple[Tuple[str], list[Document]]:\n",
    "    \"\"\"Retrieve information related to user query\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    print(f\"Serialized: {serialized}\")\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86334efa-2aab-4064-b5f7-9c6b5fac1e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from langchain_core.language_models.chat_models import BaseChatModel, LanguageModelInput\n",
    "from langchain_core.messages.base import BaseMessage, BaseMessageChunk\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from typing import Callable\n",
    "\n",
    "LLM = chat_model\n",
    "RETRIEVE_TOOL = retrieve\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include tool-call to be sent\n",
    "def query_or_respond(state: MessagesState) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "    \"\"\"Generate tool call for retrieval or respond\"\"\"\n",
    "    llm_with_tools = LLM.bind_tools([RETRIEVE_TOOL])\n",
    "    response: Runnable[LanguageModelInput, BaseMessage] = llm_with_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21513f5b-66e9-4e70-9130-a3571e1a4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tool as node\n",
    "tools = ToolNode([retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f763f6ca-b0e7-477c-933c-3d294e861327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate a response using the retrieved content\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\" Generate answer \"\"\"\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        print(f\"State: {message}\")\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\") or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "    \n",
    "    # Run\n",
    "    ai_message = LLM.invoke(prompt)\n",
    "    return {\"messages\": [ai_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f3129-6e55-43f9-b0ea-2bc10987071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile it\n",
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "graph_builder.add_node(generate)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"generate\")\n",
    "graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7aa67b-c74b-42c4-9787-1fc3f26e7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30d8ff-3b4d-478b-976d-36d3dbfb9947",
   "metadata": {},
   "source": [
    "## Testing the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a93b5-b859-4054-b85d-b4d9f1c7190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "input_message = \"Hello\"\n",
    "\n",
    "stream: Iterator[BaseMessageChunk] = graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\"\n",
    ")\n",
    "\n",
    "for step in stream:\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb298ad5-bd7e-4ca4-8433-b008150a49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51508b4d-d0af-4eb4-b6bd-295c03f9154a",
   "metadata": {},
   "source": [
    "## Stateful agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8a331-179b-45a6-9077-bb8b865c66ab",
   "metadata": {},
   "source": [
    "### Memory store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13896ef-785c-4a02-bb9c-0c7b8fac31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify threadid\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb64f1b-516f-4494-a399-eb5b022a0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"What is Task Decomposition?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a64b8-960a-40ca-8d1e-9e72e79648e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"Can you look up some common ways of doing it?\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecb8f5-600e-4dd6-8120-2125ded08e1a",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d71ff-ca53-42ed-88fa-f081c043eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(LLM, [retrieve], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04233409-64ce-4e94-8716-8538fc8e5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(agent_executor.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e92fb6-5f54-4cc5-b975-688b4cf97650",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"def234\"}}\n",
    "\n",
    "input_message = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent_executor.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "    config=config,\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aab8b2-1a4e-4549-8d84-b1cd21960aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
