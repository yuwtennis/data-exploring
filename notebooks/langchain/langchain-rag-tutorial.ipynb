{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db114ac5-c3bc-4f08-bd0e-9e9f376a0057",
   "metadata": {},
   "source": [
    "## RAG Tutorial\n",
    "\n",
    "This notebook simply follows the online doc of langchain.\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f9861-5d6b-4108-8882-57ddcd222da7",
   "metadata": {},
   "source": [
    "### Pre requisites\n",
    "\n",
    "Google API Key \n",
    "https://cloud.google.com/docs/authentication/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465dc233-a8b0-4e76-a95c-5db4d565dfe8",
   "metadata": {},
   "source": [
    "### Setting up required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394e899f-582d-4e20-a2bf-120ea1988f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51340ef-0d96-4e59-83fd-2be38cfc330c",
   "metadata": {},
   "source": [
    "### Set langsmith property for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a5b203-4ec7-4207-8778-5821731f2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = \"true\"\n",
    "os.environ['LANGSMITH_API_KEY'] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75818631-30e5-42e4-89e6-c9e23923e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key for google gemini:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for google gemini: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50bd3c36-8a26-49cb-bf0f-c4926a4fcd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter project id:  ········\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"GOOGLE_PROJECT\"] = getpass.getpass(\"Enter project id: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de7121-6dac-485f-8226-15247acff8a8",
   "metadata": {},
   "source": [
    "### Setting up components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2c72a-33fc-44be-a4bf-8cd71af38b9e",
   "metadata": {},
   "source": [
    "#### Setting up chat model (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc84bd66-717a-47fd-8798-67c625168524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[google-genai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad8f566-dd8d-4679-8031-a50a0854da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "\n",
    "llm = init_chat_model('gemini-2.0-flash', model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439021b-1c45-4eca-af74-77c06f6afd03",
   "metadata": {},
   "source": [
    "#### Setting up model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3f5b0e-95fa-428b-a220-2423b582f316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain-google-vertexai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4590fbff-fec4-44a2-af77-d567caab2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(project=os.environ.get('GOOGLE_PROJECT'), model=\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a6962-464d-4e38-b469-1f43ec190943",
   "metadata": {},
   "source": [
    "#### Setting up the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99a9c71-a576-4337-8985-cdb0564f6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e7f51-cdf6-459f-b091-6f6ad769824c",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fe850a4-dda6-4779-8f88-e10d383cb2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346f7b1-8e7b-48ad-8557-e51f6d06ba78",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f6b8f08-cd83-4137-a0d4-ecbb0beeb2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc51d4f-c250-49cb-97b4-cc0127f2edf6",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48ff2a1-0f24-4d00-92f9-5ef34b774b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecfe0fa-7119-4cbf-ad41-8f33192fd8ea",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ec9f2ba-7df9-4895-ada9-ea72b471781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df67b21-f801-4c51-8624-423c76e184ab",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "538d0b4e-e2c3-4fa9-8844-6f55ab4f5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State) -> list[Document]:\n",
    "    print(f\"Message state for retrieve step: {state}\")\n",
    "    retrieved_docs: list[Document] = vector_store.similarity_search(state[\"question\"])\n",
    "    print(retrieved_docs)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State) -> BaseMessage:\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages  = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46775286-6053-40a7-b07b-f83a4f1e6e70",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebffb0ea-cda1-4b83-b942-235bc67fe3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3a216b-0d5c-4c31-af42-fd03e4528367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message state for retrieve step: {'question': 'What is Task Decomposition?'}\n",
      "[Document(id='ff782c3a-d247-4b1b-a430-4d449e2d10e9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.'), Document(id='e88c5c38-2c7c-4819-a2d2-fe7f5b1ba1c9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#'), Document(id='731d09d0-d76e-4d14-bfb5-0676761fa0be', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",'), Document(id='1c7a77d7-46e3-4a96-acf7-c01d58c62101', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\")]\n",
      "Task decomposition involves breaking down a complex task into smaller, more manageable steps. This can be achieved through prompting the model to \"think step by step\" or by using task-specific instructions. It can also be done with human inputs or by relying on an external classical planner.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a7315-99a2-4db4-8164-75089be11528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
